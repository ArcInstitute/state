Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
Traceback (most recent call last):
  File "/home/yhr/UCE_train/train_lit.py", line 169, in <module>
    main(args)
  File "/home/yhr/UCE_train/train_lit.py", line 127, in main
    trainer.fit(model=model, train_dataloaders=dataloader)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 532, in fit
    call._call_and_handle_interrupt(
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 571, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 938, in _run
    self.strategy.setup_environment()
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 143, in setup_environment
    self.setup_distributed()
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 191, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 258, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1312, in init_process_group
    default_pg, _ = _new_process_group_helper(
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1533, in _new_process_group_helper
    backend_class = ProcessGroupNCCL(
ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
Traceback (most recent call last):
  File "/home/yhr/UCE_train/train_lit.py", line 169, in <module>
    main(args)
  File "/home/yhr/UCE_train/train_lit.py", line 127, in main
    trainer.fit(model=model, train_dataloaders=dataloader)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 532, in fit
    call._call_and_handle_interrupt(
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py", line 42, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 571, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py", line 938, in _run
    self.strategy.setup_environment()
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 143, in setup_environment
    self.setup_distributed()
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/pytorch/strategies/ddp.py", line 191, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/lightning/fabric/utilities/distributed.py", line 258, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1312, in init_process_group
    default_pg, _ = _new_process_group_helper(
  File "/home/yhr/.conda/envs/uce/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1533, in _new_process_group_helper
    backend_class = ProcessGroupNCCL(
ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!