{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# import pearson correlation\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2917127341311134"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_df  = pd.read_csv(\"/large_storage/ctc/userspace/aadduri/preprint/replogle_filtered_globalsimplesum/rpe1/eval_final.ckpt/rpe1_results.csv\")\n",
    "sets_df = pd.read_csv(\"/large_storage/ctc/userspace/aadduri/preprint/replogle_llama_21712320_filtered_copy/rpe1/eval_step=56000.ckpt/rpe1_results.csv\")\n",
    "sets_df['discrimination_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_pred_mean = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/replogle_filtered_globalsimplesum/rpe1/eval_final.ckpt/adata_pred.h5ad')\n",
    "adata_real_mean = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/replogle_filtered_globalsimplesum/rpe1/eval_final.ckpt/adata_real.h5ad')\n",
    "\n",
    "\n",
    "# mean_df['discrimination_score_rev'] = compute_perturbation_ranking_score_rev(adata_pred_mean, adata_real_mean)\n",
    "\n",
    "adata_pred = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/replogle_llama_21712320_filtered/rpe1/eval_step=56000.ckpt/adata_pred.h5ad')\n",
    "adata_real = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/replogle_llama_21712320_filtered/rpe1/eval_step=56000.ckpt/adata_real.h5ad')\n",
    "\n",
    "adata_pred_hop62 = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/tahoe_llama_58562784/holdout/eval_last.ckpt/HOP62_pred.h5ad')\n",
    "adata_real_hop62 = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/tahoe_llama_58562784/holdout/eval_last.ckpt/HOP62_real.h5ad')\n",
    "\n",
    "\n",
    "# sets_df['discrimination_score_rev'] = compute_perturbation_ranking_score_rev(adata_pred, adata_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pertsets/lib/python3.12/site-packages/matplotlib/__init__.py:968\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m _rc_params_in_file(\n\u001b[0;32m--> 968\u001b[0m     \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[1;32m    971\u001b[0m     fail_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pertsets/lib/python3.12/site-packages/matplotlib/cbook.py:545\u001b[0m, in \u001b[0;36m_get_data_path\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data_path\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    ``*args`` specify a path relative to the base data path.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(\u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_path\u001b[49m(), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming sets_df and mean_df are available\n",
    "# You may need to adjust the effect_size_column based on your data\n",
    "effect_size_column = 'de_nsig_counts_real'  # Change this to your preferred effect size column\n",
    "\n",
    "# Combine dataframes with model labels\n",
    "sets_df_labeled = sets_df.copy()\n",
    "sets_df_labeled['model'] = 'sets'\n",
    "\n",
    "mean_df_labeled = mean_df.copy()\n",
    "mean_df_labeled['model'] = 'mean'\n",
    "\n",
    "# Combine both dataframes\n",
    "combined_df = pd.concat([sets_df_labeled, mean_df_labeled], ignore_index=True)\n",
    "\n",
    "# Remove any rows with NaN values in the effect size or discrimination score columns\n",
    "combined_df = combined_df.dropna(subset=[effect_size_column, 'discrimination_score'])\n",
    "\n",
    "# Create 10 effect size bins using quantiles\n",
    "combined_df['effect_size_bin'] = pd.qcut(\n",
    "    combined_df[effect_size_column], \n",
    "    q=10, \n",
    "    labels=[f'Bin {i+1}' for i in range(10)],\n",
    "    duplicates='drop'  # Handle cases where there are duplicate bin edges\n",
    ")\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(1, 10, figsize=(40, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create box plots for each bin\n",
    "for i, bin_label in enumerate(combined_df['effect_size_bin'].cat.categories):\n",
    "    bin_data = combined_df[combined_df['effect_size_bin'] == bin_label]\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(\n",
    "        data=bin_data, \n",
    "        x='model', \n",
    "        y='discrimination_score', \n",
    "        ax=axes[i]\n",
    "    )\n",
    "    \n",
    "    # Customize the subplot\n",
    "    axes[i].set_title(f'{bin_label}\\n(n={len(bin_data)})')\n",
    "    axes[i].set_xlabel('Model')\n",
    "    axes[i].set_ylabel('Discrimination Score')\n",
    "    axes[i].set_ylim((0, 1.0))\n",
    "    \n",
    "    # Add effect size range to title\n",
    "    bin_min = bin_data[effect_size_column].min()\n",
    "    bin_max = bin_data[effect_size_column].max()\n",
    "    axes[i].set_title(f'{bin_label}\\n{effect_size_column}: [{bin_min:.3f}, {bin_max:.3f}]\\n(n={len(bin_data)})')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Discrimination Score Comparison by {effect_size_column} Bins', \n",
    "             fontsize=16, y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print summary statistics for each bin\n",
    "print(\"\\nSummary Statistics by Effect Size Bin:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for bin_label in combined_df['effect_size_bin'].cat.categories:\n",
    "    bin_data = combined_df[combined_df['effect_size_bin'] == bin_label]\n",
    "    print(f\"\\n{bin_label}:\")\n",
    "    print(f\"Effect size range: [{bin_data[effect_size_column].min():.3f}, {bin_data[effect_size_column].max():.3f}]\")\n",
    "    \n",
    "    summary = bin_data.groupby('model')['discrimination_score'].agg(['count', 'mean', 'std', 'median'])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pertsets/lib/python3.12/site-packages/matplotlib/__init__.py:968\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m _rc_params_in_file(\n\u001b[0;32m--> 968\u001b[0m     \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[1;32m    971\u001b[0m     fail_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pertsets/lib/python3.12/site-packages/matplotlib/cbook.py:545\u001b[0m, in \u001b[0;36m_get_data_path\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data_path\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    ``*args`` specify a path relative to the base data path.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(\u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_path\u001b[49m(), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming sets_df and mean_df are available\n",
    "# You may need to adjust the effect_size_column based on your data\n",
    "effect_size_column = 'de_nsig_counts_real'  # Change this to your preferred effect size column\n",
    "\n",
    "# Combine dataframes with model labels\n",
    "sets_df_labeled = sets_df.copy()\n",
    "sets_df_labeled['model'] = 'sets'\n",
    "\n",
    "mean_df_labeled = mean_df.copy()\n",
    "mean_df_labeled['model'] = 'mean'\n",
    "\n",
    "# Combine both dataframes\n",
    "combined_df = pd.concat([sets_df_labeled, mean_df_labeled], ignore_index=True)\n",
    "\n",
    "# Remove any rows with NaN values in the effect size or discrimination score columns\n",
    "combined_df = combined_df.dropna(subset=[effect_size_column, 'de_nsig_counts_pred'])\n",
    "\n",
    "# Create 10 effect size bins using quantiles\n",
    "combined_df['effect_size_bin'] = pd.qcut(\n",
    "    combined_df[effect_size_column], \n",
    "    q=10, \n",
    "    labels=[f'Bin {i+1}' for i in range(10)],\n",
    "    duplicates='drop'  # Handle cases where there are duplicate bin edges\n",
    ")\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(1, 10, figsize=(40, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create box plots for each bin\n",
    "for i, bin_label in enumerate(combined_df['effect_size_bin'].cat.categories):\n",
    "    bin_data = combined_df[combined_df['effect_size_bin'] == bin_label]\n",
    "    \n",
    "    # Create box plot\n",
    "    sns.boxplot(\n",
    "        data=bin_data, \n",
    "        x='model', \n",
    "        y='de_nsig_counts_pred', \n",
    "        ax=axes[i]\n",
    "    )\n",
    "    \n",
    "    # Customize the subplot\n",
    "    axes[i].set_title(f'{bin_label}\\n(n={len(bin_data)})')\n",
    "    axes[i].set_xlabel('Model')\n",
    "    axes[i].set_ylabel('Discrimination Score')\n",
    "    # axes[i].set_ylim((0, 1.0))\n",
    "    \n",
    "    # Add effect size range to title\n",
    "    bin_min = bin_data[effect_size_column].min()\n",
    "    bin_max = bin_data[effect_size_column].max()\n",
    "    axes[i].set_title(f'{bin_label}\\n{effect_size_column}: [{bin_min:.3f}, {bin_max:.3f}]\\n(n={len(bin_data)})')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Discrimination Score Comparison by {effect_size_column} Bins', \n",
    "             fontsize=16, y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print summary statistics for each bin\n",
    "print(\"\\nSummary Statistics by Effect Size Bin:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for bin_label in combined_df['effect_size_bin'].cat.categories:\n",
    "    bin_data = combined_df[combined_df['effect_size_bin'] == bin_label]\n",
    "    print(f\"\\n{bin_label}:\")\n",
    "    print(f\"Effect size range: [{bin_data[effect_size_column].min():.3f}, {bin_data[effect_size_column].max():.3f}]\")\n",
    "    \n",
    "    summary = bin_data.groupby('model')['discrimination_score'].agg(['count', 'mean', 'std', 'median'])\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to PCA before discrim score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3173493/2352431115.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3173493/2352431115.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3173493/2352431115.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3173493/2352431115.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.24906379929476255,\n",
       " array([0.72032193, 0.83702213, 0.        , 0.01810865, 0.88732394,\n",
       "        0.        , 0.00603622, 0.14486922, 0.        , 0.43863179,\n",
       "        0.23742455, 0.55734406, 0.32394366, 0.25150905, 0.24547284,\n",
       "        0.05030181, 0.        , 0.27162978, 0.20321932, 0.00804829,\n",
       "        0.        , 0.00201207, 0.        , 0.01810865, 0.30985915,\n",
       "        0.01609658, 0.2193159 , 0.61569416, 0.01810865, 0.54325956,\n",
       "        0.1167002 , 0.        , 0.30382294, 0.58752515, 0.58752515,\n",
       "        0.        , 0.68209256, 0.        , 0.23943662, 0.14486922,\n",
       "        0.28370221, 0.19919517, 0.07042254, 0.06438632, 0.01609658,\n",
       "        0.11267606, 0.10060362, 0.01609658, 0.        , 0.61971831,\n",
       "        0.        , 0.31589537, 0.52917505, 0.0221328 , 0.74647887,\n",
       "        0.00603622, 0.18309859, 0.02012072, 0.01408451, 0.01609658,\n",
       "        0.00402414, 0.        , 0.0221328 , 0.11267606, 0.        ,\n",
       "        0.16297787, 0.47887324, 0.00201207, 0.        , 0.7665996 ,\n",
       "        0.0221328 , 0.03822938, 0.04225352, 0.        , 0.66197183,\n",
       "        0.43661972, 0.24547284, 0.42052314, 0.00603622, 0.        ,\n",
       "        0.        , 0.        , 0.22736419, 0.06237425, 0.138833  ,\n",
       "        0.85110664, 0.69818913, 0.00603622, 0.02414487, 0.00603622,\n",
       "        0.        , 0.55734406, 0.00201207, 0.16096579, 0.02012072,\n",
       "        0.42253521, 0.36418511, 0.15090543, 0.44466801, 0.06639839,\n",
       "        0.00402414, 0.29577465, 0.3360161 , 0.        , 0.20120724,\n",
       "        0.00603622, 0.02816901, 0.49295775, 0.84708249, 0.13682093,\n",
       "        0.04426559, 0.01609658, 0.74849095, 0.66599598, 0.17102616,\n",
       "        0.01609658, 0.        , 0.        , 0.00402414, 0.00201207,\n",
       "        0.00402414, 0.17907445, 0.48692153, 0.        , 0.18309859,\n",
       "        0.02414487, 0.07645875, 0.00201207, 0.00201207, 0.04024145,\n",
       "        0.09255533, 0.00201207, 0.01006036, 0.10060362, 0.12877264,\n",
       "        0.        , 0.        , 0.14688129, 0.10060362, 0.84708249,\n",
       "        0.09255533, 0.03219316, 0.00402414, 0.0804829 , 0.01810865,\n",
       "        0.19517103, 0.        , 0.00804829, 0.        , 0.66599598,\n",
       "        0.89336016, 0.00201207, 0.18712274, 0.09255533, 0.14688129,\n",
       "        0.4084507 , 0.20321932, 0.13682093, 0.08249497, 0.66197183,\n",
       "        0.18309859, 0.        , 0.12877264, 0.33400402, 0.67203219,\n",
       "        0.10060362, 0.00402414, 0.        , 0.00201207, 0.07042254,\n",
       "        0.04828974, 0.00603622, 0.23138833, 0.07847082, 0.        ,\n",
       "        0.        , 0.03018109, 0.09255533, 0.10663984, 0.09859155,\n",
       "        0.        , 0.80885312, 0.61569416, 0.7665996 , 0.05231388,\n",
       "        0.21529175, 0.03018109, 0.02816901, 0.85714286, 0.03219316,\n",
       "        0.        , 0.        , 0.48490946, 0.05030181, 0.5110664 ,\n",
       "        0.01408451, 0.41046278, 0.01609658, 0.10462777, 0.01207243,\n",
       "        0.20120724, 0.36016097, 0.0583501 , 0.01207243, 0.00804829,\n",
       "        0.0221328 , 0.01810865, 0.00603622, 0.05432596, 0.        ,\n",
       "        0.00804829, 0.        , 0.10261569, 0.15492958, 0.08853119,\n",
       "        0.14084507, 0.00402414, 0.79275654, 0.30382294, 0.65392354,\n",
       "        0.04828974, 0.93360161, 0.96177062, 0.74647887, 0.01810865,\n",
       "        0.64587525, 0.79678068, 0.        , 0.27364185, 0.38229376,\n",
       "        0.40241449, 0.88933602, 0.03219316, 0.92555332, 0.        ,\n",
       "        0.2193159 , 0.13279678, 0.55331992, 0.40241449, 0.05030181,\n",
       "        0.03219316, 0.        , 0.        , 0.07847082, 0.40643863,\n",
       "        0.83903421, 0.00201207, 0.00603622, 0.3943662 , 0.58551308,\n",
       "        0.03822938, 0.04627767, 0.01207243, 0.03018109, 0.06036217,\n",
       "        0.53521127, 0.27565392, 0.14688129, 0.30382294, 0.23742455,\n",
       "        0.        , 0.01609658, 0.6861167 , 0.41448692, 0.66599598,\n",
       "        0.02012072, 0.        , 0.36619718, 0.27162978, 0.03822938,\n",
       "        0.16498994, 0.09054326, 0.        , 0.39235412, 0.34406439,\n",
       "        0.61569416, 0.39839034, 0.        , 0.02615694, 0.00402414,\n",
       "        0.        , 0.62173038, 0.22535211, 0.03018109, 0.40442656,\n",
       "        0.01609658, 0.04627767, 0.0804829 , 0.00201207, 0.02615694,\n",
       "        0.91146881, 0.01408451, 0.06639839, 0.51911469, 0.02012072,\n",
       "        0.04225352, 0.23742455, 0.01609658, 0.28169014, 0.37424547,\n",
       "        0.42052314, 0.38631791, 0.13682093, 0.0221328 , 0.02816901,\n",
       "        0.        , 0.37826962, 0.00201207, 0.58551308, 0.01408451,\n",
       "        0.58350101, 0.        , 0.01006036, 0.04024145, 0.18108652,\n",
       "        0.6639839 , 0.        , 0.0221328 , 0.39034205, 0.        ,\n",
       "        0.39839034, 0.28571429, 0.28370221, 0.87122736, 0.03018109,\n",
       "        0.        , 0.5472837 , 0.63983903, 0.47484909, 0.06036217,\n",
       "        0.07847082, 0.09657948, 0.01006036, 0.30181087, 0.26358149,\n",
       "        0.83098592, 0.00804829, 0.00603622, 0.17706237, 0.58350101,\n",
       "        0.31991952, 0.0362173 , 0.02414487, 0.02615694, 0.51911469,\n",
       "        0.04627767, 0.        , 0.12474849, 0.06036217, 0.26156942,\n",
       "        0.03219316, 0.03420523, 0.277666  , 0.04828974, 0.07042254,\n",
       "        0.04024145, 0.4305835 , 0.00201207, 0.00402414, 0.03018109,\n",
       "        0.04627767, 0.01207243, 0.0221328 , 0.06036217, 0.13078471,\n",
       "        0.00603622, 0.        , 0.22334004, 0.09255533, 0.21529175,\n",
       "        0.17303823, 0.06036217, 0.        , 0.        , 0.02012072,\n",
       "        0.00201207, 0.07444668, 0.00402414, 0.        , 0.        ,\n",
       "        0.07847082, 0.00603622, 0.        , 0.67002012, 0.722334  ,\n",
       "        0.84507042, 0.        , 0.01609658, 0.34607646, 0.        ,\n",
       "        0.04426559, 0.47887324, 0.05432596, 0.38229376, 0.36217304,\n",
       "        0.02816901, 0.00201207, 0.        , 0.        , 0.07645875,\n",
       "        0.51710262, 0.06036217, 0.02012072, 0.04828974, 0.23541247,\n",
       "        0.05030181, 0.01006036, 0.01408451, 0.        , 0.        ,\n",
       "        0.64386318, 0.21126761, 0.17102616, 0.        , 0.        ,\n",
       "        0.        , 0.00201207, 0.        , 0.95573441, 0.01609658,\n",
       "        0.21730382, 0.0362173 , 0.        , 0.03219316, 0.06036217,\n",
       "        0.01207243, 0.        , 0.07645875, 0.01810865, 0.40241449,\n",
       "        0.14688129, 0.0583501 , 0.22535211, 0.06841046, 0.        ,\n",
       "        0.78672032, 0.33400402, 0.89939638, 0.        , 0.00603622,\n",
       "        0.21126761, 0.69014085, 0.28973843, 0.00804829, 0.        ,\n",
       "        0.31991952, 0.52515091, 0.16498994, 0.04024145, 0.        ,\n",
       "        0.0362173 , 0.24346076, 0.97384306, 0.03219316, 0.27967807,\n",
       "        0.01207243, 0.14084507, 0.26760563, 0.08450704, 0.24949698,\n",
       "        0.11468813, 0.62374245, 0.00603622, 0.        , 0.17303823,\n",
       "        0.        , 0.08853119, 0.14486922, 0.6277666 , 0.59758551,\n",
       "        0.00402414, 0.09859155, 0.07243461, 0.05432596, 0.07243461,\n",
       "        0.0221328 , 0.27565392, 0.14889336, 0.29979879, 0.4084507 ,\n",
       "        0.        , 0.00603622, 0.01810865, 0.        , 0.33199195,\n",
       "        0.        , 0.        , 0.14084507, 0.        , 0.20724346,\n",
       "        0.23138833, 0.        , 0.277666  , 0.49295775, 0.93561368,\n",
       "        0.38028169]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_mean_perturbation_effect(adata, pert_col=\"gene\", ctrl_pert=\"non-targeting\"):\n",
    "    adata_df = adata.to_df()\n",
    "    adata_df[\"pert\"] = adata.obs[pert_col].values\n",
    "    mean_df = adata_df.groupby(\"pert\").mean()\n",
    "    mean_pert_effect = np.abs(mean_df - mean_df.loc[ctrl_pert])\n",
    "    return mean_pert_effect\n",
    "\n",
    "\n",
    "def compute_perturbation_id_score(adata_pred, adata_real, pert_col=\"gene\", ctrl_pert=\"non-targeting\"):\n",
    "    ## Given a specific perturbation, identify which model-predicted\n",
    "    # perturbation is most similar to it\n",
    "\n",
    "    ## Compute true mean perturbation effect\n",
    "    mean_real_effect = compute_mean_perturbation_effect(adata_real, pert_col, ctrl_pert)\n",
    "    mean_pred_effect = compute_mean_perturbation_effect(adata_pred, pert_col, ctrl_pert)\n",
    "    all_perts = mean_real_effect.index.values\n",
    "    ## For each true perturbation effect, find the nearest neighbor predicted\n",
    "    # perturbation effect\n",
    "\n",
    "    pred_perts = []\n",
    "    for pert in all_perts:\n",
    "        real_effect = mean_real_effect.loc[pert].values\n",
    "        pred_effects = mean_pred_effect.values\n",
    "        pred_pert = all_perts[np.argmax(cosine_similarity(real_effect.reshape(1, -1), pred_effects))]\n",
    "        pred_perts.append(pred_pert)\n",
    "\n",
    "    accuracy_score = np.sum(pred_perts == all_perts) / len(all_perts)\n",
    "\n",
    "    return accuracy_score\n",
    "\n",
    "\n",
    "def compute_perturbation_ranking_score(adata_pred, adata_real, pert_col=\"gene\", ctrl_pert=\"non-targeting\"):\n",
    "    ## Compute true mean perturbation effect\n",
    "    mean_real_effect = compute_mean_perturbation_effect(adata_real, pert_col, ctrl_pert)\n",
    "    mean_pred_effect = compute_mean_perturbation_effect(adata_pred, pert_col, ctrl_pert)\n",
    "    all_perts = mean_real_effect.index.values\n",
    "\n",
    "    ranks = []\n",
    "\n",
    "    ## For each true perturbation effect, compute similarity to all predicted effects\n",
    "    for pert in all_perts:\n",
    "        real_effect = mean_real_effect.loc[pert].values.reshape(1, -1)\n",
    "        pred_effects = mean_pred_effect.values\n",
    "\n",
    "        # Compute cosine similarities between the real effect and all predicted effects\n",
    "        similarities = cosine_similarity(real_effect, pred_effects).flatten()\n",
    "\n",
    "        # Get the rank of the true perturbation based on similarity\n",
    "        true_pert_index = np.where(all_perts == pert)[0][0]\n",
    "        sorted_indices = np.argsort(similarities)[::-1]  # Sort in descending order of similarity\n",
    "        rank_of_true_pert = np.where(sorted_indices == true_pert_index)[0][0]  # 1-based rank\n",
    "\n",
    "        ranks.append(rank_of_true_pert)\n",
    "\n",
    "    ## mean normalized rank\n",
    "    mean_rank = np.mean(ranks) / len(all_perts)\n",
    "\n",
    "    return mean_rank\n",
    "\n",
    "def compute_perturbation_ranking_score_rev(adata_pred, adata_real, pert_col=\"gene\", ctrl_pert=\"non-targeting\"):\n",
    "    ## Compute true mean perturbation effect\n",
    "    mean_real_effect = compute_mean_perturbation_effect(adata_real, pert_col, ctrl_pert)\n",
    "    mean_pred_effect = compute_mean_perturbation_effect(adata_pred, pert_col, ctrl_pert)\n",
    "    all_perts = mean_pred_effect.index.values  # Changed: now iterating over predicted perturbations\n",
    "    ranks = []\n",
    "    ## For each predicted perturbation effect, compute similarity to all real effects\n",
    "    for pert in all_perts:\n",
    "        if pert == ctrl_pert:\n",
    "            continue\n",
    "        pred_effect = mean_pred_effect.loc[pert].values.reshape(1, -1)  # Changed: get pred effect\n",
    "        real_effects = mean_real_effect.values  # Changed: compare against all real effects\n",
    "        # Compute cosine similarities between the predicted effect and all real effects\n",
    "        similarities = cosine_similarity(pred_effect, real_effects).flatten()\n",
    "        # Get the rank of the true perturbation based on similarity\n",
    "        true_pert_index = np.where(all_perts == pert)[0][0]\n",
    "        sorted_indices = np.argsort(similarities)[::-1]  # Sort in descending order of similarity\n",
    "        rank_of_true_pert = np.where(sorted_indices == true_pert_index)[0][0]  # 1-based rank\n",
    "        ranks.append(rank_of_true_pert)\n",
    "    ## mean normalized rank\n",
    "    return np.array(ranks) / len(all_perts)\n",
    "\n",
    "compute_perturbation_ranking_score(adata_pred_mean, adata_real_mean), compute_perturbation_ranking_score_rev(adata_pred_mean, adata_real_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_pred = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/replogle_llama_21712320_filtered/rpe1/eval_step=56000.ckpt/adata_pred.h5ad')\n",
    "adata_real = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/replogle_llama_21712320_filtered/rpe1/eval_step=56000.ckpt/adata_real.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy but with .obsm['X_vci_1.5.2_4'] in the .X\n",
    "adata_pred_emb_min = ad.AnnData(X=adata_pred.obsm['X_vci_1.5.2_4'][:, -10:], obs=adata_pred.obs)\n",
    "adata_real_emb_min = ad.AnnData(X=adata_real.obsm['X_vci_1.5.2_4'][:, -10:], obs=adata_real.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SHAPES ===\n",
      "adata_pred.X shape: (49389, 2000)\n",
      "adata_real.X shape: (49389, 2000)\n",
      "adata_pred_emb.X shape: (49389, 2058)\n",
      "adata_real_emb.X shape: (49389, 2058)\n",
      "\n",
      "=== RAW DATA STATISTICS ===\n",
      "PREDICTED (raw):\n",
      "  Mean: 0.459961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruvgautam/miniconda/envs/pertsets/lib/python3.12/site-packages/numpy/core/_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Std: inf\n",
      "  Min: 0.000000\n",
      "  Max: 9.234375\n",
      "REAL (raw):\n",
      "  Mean: 0.467529\n",
      "  Std: inf\n",
      "  Min: 0.000000\n",
      "  Max: 7.187500\n",
      "\n",
      "=== EMBEDDING STATISTICS ===\n",
      "PREDICTED (embeddings):\n",
      "  Mean: -0.000330\n",
      "  Std: 0.017960\n",
      "  Min: -0.935059\n",
      "  Max: 0.864746\n",
      "REAL (embeddings):\n",
      "  Mean: -0.000328\n",
      "  Std: 0.017960\n",
      "  Min: -0.877930\n",
      "  Max: 0.866699\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the basic properties of the data\n",
    "print(\"=== DATA SHAPES ===\")\n",
    "print(f\"adata_pred.X shape: {adata_pred.X.shape}\")\n",
    "print(f\"adata_real.X shape: {adata_real.X.shape}\")\n",
    "print(f\"adata_pred_emb.X shape: {adata_pred_emb.X.shape}\")\n",
    "print(f\"adata_real_emb.X shape: {adata_real_emb.X.shape}\")\n",
    "\n",
    "print(\"\\n=== RAW DATA STATISTICS ===\")\n",
    "print(\"PREDICTED (raw):\")\n",
    "print(f\"  Mean: {adata_pred.X.mean():.6f}\")\n",
    "print(f\"  Std: {adata_pred.X.std():.6f}\")\n",
    "print(f\"  Min: {adata_pred.X.min():.6f}\")\n",
    "print(f\"  Max: {adata_pred.X.max():.6f}\")\n",
    "\n",
    "print(\"REAL (raw):\")\n",
    "print(f\"  Mean: {adata_real.X.mean():.6f}\")\n",
    "print(f\"  Std: {adata_real.X.std():.6f}\")\n",
    "print(f\"  Min: {adata_real.X.min():.6f}\")\n",
    "print(f\"  Max: {adata_real.X.max():.6f}\")\n",
    "\n",
    "print(\"\\n=== EMBEDDING STATISTICS ===\")\n",
    "print(\"PREDICTED (embeddings):\")\n",
    "print(f\"  Mean: {adata_pred_emb.X.mean():.6f}\")\n",
    "print(f\"  Std: {adata_pred_emb.X.std():.6f}\")\n",
    "print(f\"  Min: {adata_pred_emb.X.min():.6f}\")\n",
    "print(f\"  Max: {adata_pred_emb.X.max():.6f}\")\n",
    "\n",
    "print(\"REAL (embeddings):\")\n",
    "print(f\"  Mean: {adata_real_emb.X.mean():.6f}\")\n",
    "print(f\"  Std: {adata_real_emb.X.std():.6f}\")\n",
    "print(f\"  Min: {adata_real_emb.X.min():.6f}\")\n",
    "print(f\"  Max: {adata_real_emb.X.max():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAMPLE VECTORS (first 10 features) ===\n",
      "PREDICTED (raw) - first sample:\n",
      "[0.4778 0.     0.     0.2445 0.     3.316  0.2078 0.5366 0.     0.    ]\n",
      "\n",
      "REAL (raw) - first sample:\n",
      "[0.    1.013 0.    0.    0.    3.092 0.    1.505 0.    0.   ]\n",
      "\n",
      "PREDICTED (embeddings) - first sample:\n",
      "[-0.002844  -0.001372   0.012566  -0.0296     0.02783   -0.03595\n",
      "  0.00719    0.00575    0.0202     0.0002565]\n",
      "\n",
      "REAL (embeddings) - first sample:\n",
      "[ 0.00928  -0.003614  0.00543  -0.04504   0.01735  -0.02655   0.0176\n",
      "  0.002775  0.0321    0.02696 ]\n",
      "\n",
      "=== VECTOR MAGNITUDES (L2 norm) ===\n",
      "RAW DATA L2 norms:\n",
      "  Predicted - mean: 38.250000, std: inf\n",
      "  Real - mean: 39.062500, std: inf\n",
      "EMBEDDING L2 norms:\n",
      "  Predicted - mean: 1.647461, std: inf\n",
      "  Real - mean: 1.675781, std: inf\n"
     ]
    }
   ],
   "source": [
    "# Let's look at some actual vectors\n",
    "print(\"=== SAMPLE VECTORS (first 10 features) ===\")\n",
    "print(\"PREDICTED (raw) - first sample:\")\n",
    "print(adata_pred.X[0, :10].toarray() if hasattr(adata_pred.X, 'toarray') else adata_pred.X[0, :10])\n",
    "\n",
    "print(\"\\nREAL (raw) - first sample:\")\n",
    "print(adata_real.X[0, :10].toarray() if hasattr(adata_real.X, 'toarray') else adata_real.X[0, :10])\n",
    "\n",
    "print(\"\\nPREDICTED (embeddings) - first sample:\")\n",
    "print(adata_pred_emb.X[0, :10])\n",
    "\n",
    "print(\"\\nREAL (embeddings) - first sample:\")\n",
    "print(adata_real_emb.X[0, :10])\n",
    "\n",
    "print(\"\\n=== VECTOR MAGNITUDES (L2 norm) ===\")\n",
    "import numpy as np\n",
    "\n",
    "# Calculate L2 norms for raw data\n",
    "pred_raw_norms = np.linalg.norm(adata_pred.X.toarray() if hasattr(adata_pred.X, 'toarray') else adata_pred.X, axis=1)\n",
    "real_raw_norms = np.linalg.norm(adata_real.X.toarray() if hasattr(adata_real.X, 'toarray') else adata_real.X, axis=1)\n",
    "\n",
    "# Calculate L2 norms for embeddings\n",
    "pred_emb_norms = np.linalg.norm(adata_pred_emb.X, axis=1)\n",
    "real_emb_norms = np.linalg.norm(adata_real_emb.X, axis=1)\n",
    "\n",
    "print(\"RAW DATA L2 norms:\")\n",
    "print(f\"  Predicted - mean: {pred_raw_norms.mean():.6f}, std: {pred_raw_norms.std():.6f}\")\n",
    "print(f\"  Real - mean: {real_raw_norms.mean():.6f}, std: {real_raw_norms.std():.6f}\")\n",
    "\n",
    "print(\"EMBEDDING L2 norms:\")\n",
    "print(f\"  Predicted - mean: {pred_emb_norms.mean():.6f}, std: {pred_emb_norms.std():.6f}\")\n",
    "print(f\"  Real - mean: {real_emb_norms.mean():.6f}, std: {real_emb_norms.std():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_perturbation_effect(adata, pert_col=\"gene\", ctrl_pert=\"non-targeting\"):\n",
    "    adata_df = adata.to_df()\n",
    "    adata_df[\"pert\"] = adata.obs[pert_col].values\n",
    "    mean_df = adata_df.groupby(\"pert\").mean()\n",
    "    mean_pert_effect = np.abs(mean_df - mean_df.loc[ctrl_pert])\n",
    "    return mean_pert_effect\n",
    "\n",
    "def compute_perturbation_id_score(adata_pred, adata_real, pert_col=\"gene\", ctrl_pert=\"non-targeting\"):\n",
    "    # Compute true and predicted mean effects\n",
    "    mean_real = compute_mean_perturbation_effect(adata_real, pert_col, ctrl_pert)\n",
    "    mean_pred = compute_mean_perturbation_effect(adata_pred, pert_col, ctrl_pert)\n",
    "    perts = mean_real.index.values\n",
    "\n",
    "    pred_perts = []\n",
    "    for pert in perts:\n",
    "        real_eff = mean_real.loc[pert].values\n",
    "        pred_effs = mean_pred.values\n",
    "        # Euclidean distances to all predicted effects\n",
    "        distances = np.linalg.norm(pred_effs - real_eff, axis=1)\n",
    "        # pick the predicted perturbation with smallest distance\n",
    "        pred_perts.append(perts[np.argmin(distances)])\n",
    "\n",
    "    # accuracy\n",
    "    return np.mean(np.array(pred_perts) == perts)\n",
    "\n",
    "def compute_perturbation_ranking_score(adata_pred, adata_real, pert_col=\"gene\", ctrl_pert=\"non-targeting\"):\n",
    "    mean_real = compute_mean_perturbation_effect(adata_real, pert_col, ctrl_pert)\n",
    "    mean_pred = compute_mean_perturbation_effect(adata_pred, pert_col, ctrl_pert)\n",
    "    perts = mean_real.index.values\n",
    "    n = len(perts)\n",
    "\n",
    "    ranks = []\n",
    "    for i, pert in enumerate(perts):\n",
    "        real_eff = mean_real.loc[pert].values\n",
    "        pred_effs = mean_pred.values\n",
    "        # distances\n",
    "        distances = np.linalg.norm(pred_effs - real_eff, axis=1)\n",
    "        # sort ascending → lowest distance = best match\n",
    "        sorted_idx = np.argsort(distances)\n",
    "        # find rank of the true perturbation\n",
    "        rank = np.where(sorted_idx == i)[0][0]\n",
    "        ranks.append(rank)\n",
    "\n",
    "    # normalized mean rank\n",
    "    return np.mean(ranks) / n\n",
    "\n",
    "def compute_perturbation_ranking_score_rev(adata_pred, adata_real, pert_col=\"gene\", ctrl_pert=\"non-targeting\"):\n",
    "    mean_real = compute_mean_perturbation_effect(adata_real, pert_col, ctrl_pert)\n",
    "    mean_pred = compute_mean_perturbation_effect(adata_pred, pert_col, ctrl_pert)\n",
    "    perts = mean_pred.index.values\n",
    "    n = len(perts)\n",
    "\n",
    "    ranks = []\n",
    "    for i, pert in enumerate(perts):\n",
    "        if pert == ctrl_pert:\n",
    "            continue\n",
    "        pred_eff = mean_pred.loc[pert].values\n",
    "        real_effs = mean_real.values\n",
    "        distances = np.linalg.norm(real_effs - pred_eff, axis=1)\n",
    "        sorted_idx = np.argsort(distances)\n",
    "        # true index in real_effs corresponds to pert’s position in mean_real.index\n",
    "        true_idx = np.where(mean_real.index.values == pert)[0][0]\n",
    "        rank = np.where(sorted_idx == true_idx)[0][0]\n",
    "        ranks.append(rank)\n",
    "\n",
    "    return np.array(ranks) / n\n",
    "\n",
    "def create_cosine_similarity_matrix_of_all_reals(adata_real):\n",
    "    # compute the cosine similarity between all real perturbations\n",
    "    cosine_sim = np.zeros((adata_real.shape[0], adata_real.shape[0]))\n",
    "    for i in range(adata_real.shape[0]):\n",
    "        for j in range(adata_real.shape[0]):\n",
    "            cosine_sim[i, j] = np.dot(adata_real[i], adata_real[j]) / (np.linalg.norm(adata_real[i]) * np.linalg.norm(adata_real[j]))\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 32.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcreate_cosine_similarity_matrix_of_all_reals\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata_real\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 73\u001b[0m, in \u001b[0;36mcreate_cosine_similarity_matrix_of_all_reals\u001b[0;34m(adata_real)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(adata_real\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(adata_real\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 73\u001b[0m         cosine_sim[i, j] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata_real\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madata_real\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(adata_real[i]) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(adata_real[j]))\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cosine_sim\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 32."
     ]
    }
   ],
   "source": [
    "create_cosine_similarity_matrix_of_all_reals(adata_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fix matplotlib and create distribution plots\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[1;32m      3\u001b[0m matplotlib\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgg\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Use non-interactive backend\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pertsets/lib/python3.12/site-packages/matplotlib/__init__.py:968\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m _rc_params_in_file(\n\u001b[0;32m--> 968\u001b[0m     \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[1;32m    971\u001b[0m     fail_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pertsets/lib/python3.12/site-packages/matplotlib/cbook.py:545\u001b[0m, in \u001b[0;36m_get_data_path\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data_path\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03m    ``*args`` specify a path relative to the base data path.\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(\u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_path\u001b[49m(), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "# Fix matplotlib and create distribution plots\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create subplots for distributions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Raw data value distributions\n",
    "axes[0, 0].hist(adata_pred.X.toarray().flatten() if hasattr(adata_pred.X, 'toarray') else adata_pred.X.flatten(), \n",
    "                bins=100, alpha=0.7, density=True, color='red', label='Predicted')\n",
    "axes[0, 0].set_title('Raw Data Values - Predicted')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "axes[0, 1].hist(adata_real.X.toarray().flatten() if hasattr(adata_real.X, 'toarray') else adata_real.X.flatten(), \n",
    "                bins=100, alpha=0.7, density=True, color='blue', label='Real')\n",
    "axes[0, 1].set_title('Raw Data Values - Real')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# Embedding value distributions\n",
    "axes[0, 2].hist(adata_pred_emb.X.flatten(), bins=100, alpha=0.7, density=True, color='red')\n",
    "axes[0, 2].set_title('Embedding Values - Predicted')\n",
    "axes[0, 2].set_xlabel('Value')\n",
    "axes[0, 2].set_ylabel('Density')\n",
    "\n",
    "axes[0, 3].hist(adata_real_emb.X.flatten(), bins=100, alpha=0.7, density=True, color='blue')\n",
    "axes[0, 3].set_title('Embedding Values - Real')\n",
    "axes[0, 3].set_xlabel('Value')\n",
    "axes[0, 3].set_ylabel('Density')\n",
    "\n",
    "# L2 norm distributions\n",
    "axes[1, 0].hist(pred_raw_norms, bins=50, alpha=0.7, density=True, color='red')\n",
    "axes[1, 0].set_title('L2 Norms - Raw Data Predicted')\n",
    "axes[1, 0].set_xlabel('L2 Norm')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "\n",
    "axes[1, 1].hist(real_raw_norms, bins=50, alpha=0.7, density=True, color='blue')\n",
    "axes[1, 1].set_title('L2 Norms - Raw Data Real')\n",
    "axes[1, 1].set_xlabel('L2 Norm')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "\n",
    "axes[1, 2].hist(pred_emb_norms, bins=50, alpha=0.7, density=True, color='red')\n",
    "axes[1, 2].set_title('L2 Norms - Embeddings Predicted')\n",
    "axes[1, 2].set_xlabel('L2 Norm')\n",
    "axes[1, 2].set_ylabel('Density')\n",
    "\n",
    "axes[1, 3].hist(real_emb_norms, bins=50, alpha=0.7, density=True, color='blue')\n",
    "axes[1, 3].set_title('L2 Norms - Embeddings Real')\n",
    "axes[1, 3].set_xlabel('L2 Norm')\n",
    "axes[1, 3].set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribution_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved distribution plots to 'distribution_comparison.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create side-by-side comparison plots\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Raw data comparison\u001b[39;00m\n\u001b[1;32m      5\u001b[0m pred_raw_flat \u001b[38;5;241m=\u001b[39m adata_pred\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(adata_pred\u001b[38;5;241m.\u001b[39mX, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m adata_pred\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Create side-by-side comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Raw data comparison\n",
    "pred_raw_flat = adata_pred.X.toarray().flatten() if hasattr(adata_pred.X, 'toarray') else adata_pred.X.flatten()\n",
    "real_raw_flat = adata_real.X.toarray().flatten() if hasattr(adata_real.X, 'toarray') else adata_real.X.flatten()\n",
    "\n",
    "axes[0, 0].hist(pred_raw_flat, bins=100, alpha=0.6, density=True, color='red', label='Predicted')\n",
    "axes[0, 0].hist(real_raw_flat, bins=100, alpha=0.6, density=True, color='blue', label='Real')\n",
    "axes[0, 0].set_title('Raw Data Values Comparison')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Embedding comparison\n",
    "axes[0, 1].hist(adata_pred_emb.X.flatten(), bins=100, alpha=0.6, density=True, color='red', label='Predicted')\n",
    "axes[0, 1].hist(adata_real_emb.X.flatten(), bins=100, alpha=0.6, density=True, color='blue', label='Real')\n",
    "axes[0, 1].set_title('Embedding Values Comparison')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# L2 norm comparison - Raw\n",
    "axes[1, 0].hist(pred_raw_norms, bins=50, alpha=0.6, density=True, color='red', label='Predicted')\n",
    "axes[1, 0].hist(real_raw_norms, bins=50, alpha=0.6, density=True, color='blue', label='Real')\n",
    "axes[1, 0].set_title('L2 Norms - Raw Data Comparison')\n",
    "axes[1, 0].set_xlabel('L2 Norm')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# L2 norm comparison - Embeddings\n",
    "axes[1, 1].hist(pred_emb_norms, bins=50, alpha=0.6, density=True, color='red', label='Predicted')\n",
    "axes[1, 1].hist(real_emb_norms, bins=50, alpha=0.6, density=True, color='blue', label='Real')\n",
    "axes[1, 1].set_title('L2 Norms - Embeddings Comparison')\n",
    "axes[1, 1].set_xlabel('L2 Norm')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('side_by_side_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved side-by-side comparison plots to 'side_by_side_comparison.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED PERCENTILE ANALYSIS ===\n",
      "\n",
      "RAW DATA PERCENTILES:\n",
      "Predicted:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred_raw_flat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m percentiles:\n\u001b[0;32m----> 9\u001b[0m     val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(\u001b[43mpred_raw_flat\u001b[49m, p)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m5.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m12.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_raw_flat' is not defined"
     ]
    }
   ],
   "source": [
    "# Detailed percentile analysis\n",
    "print(\"=== DETAILED PERCENTILE ANALYSIS ===\")\n",
    "\n",
    "percentiles = [0.1, 1, 5, 10, 25, 50, 75, 90, 95, 99, 99.9]\n",
    "\n",
    "print(\"\\nRAW DATA PERCENTILES:\")\n",
    "print(\"Predicted:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(pred_raw_flat, p)\n",
    "    print(f\"  {p:5.1f}%: {val:12.6f}\")\n",
    "\n",
    "print(\"Real:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(real_raw_flat, p)\n",
    "    print(f\"  {p:5.1f}%: {val:12.6f}\")\n",
    "\n",
    "print(\"\\nEMBEDDING PERCENTILES:\")\n",
    "print(\"Predicted:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(adata_pred_emb.X.flatten(), p)\n",
    "    print(f\"  {p:5.1f}%: {val:12.6f}\")\n",
    "\n",
    "print(\"Real:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(adata_real_emb.X.flatten(), p)\n",
    "    print(f\"  {p:5.1f}%: {val:12.6f}\")\n",
    "\n",
    "print(\"\\nL2 NORM PERCENTILES:\")\n",
    "print(\"Raw Data - Predicted:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(pred_raw_norms, p)\n",
    "    print(f\"  {p:5.1f}%: {val:12.6f}\")\n",
    "\n",
    "print(\"Raw Data - Real:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(real_raw_norms, p)\n",
    "    print(f\"  {p:5.1f}%: {val:12.6f}\")\n",
    "\n",
    "print(\"Embeddings - Predicted:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(pred_emb_norms, p)\n",
    "    print(f\"  {p:5.1f}%: {val:12.6f}\")\n",
    "\n",
    "print(\"Embeddings - Real:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(real_emb_norms, p)\n",
    "    print(f\"  {p:5.1f}%: {val:12.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create box plots for better visualization of distributions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Sample random subset for box plots (to avoid memory issues)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m n_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(pred_raw_flat))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Create box plots for better visualization of distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sample random subset for box plots (to avoid memory issues)\n",
    "n_sample = min(10000, len(pred_raw_flat))\n",
    "idx_sample = np.random.choice(len(pred_raw_flat), n_sample, replace=False)\n",
    "\n",
    "# Raw data box plot\n",
    "raw_data_for_box = pd.DataFrame({\n",
    "    'Value': np.concatenate([pred_raw_flat[idx_sample], real_raw_flat[idx_sample]]),\n",
    "    'Type': ['Predicted']*n_sample + ['Real']*n_sample\n",
    "})\n",
    "\n",
    "sns.boxplot(data=raw_data_for_box, x='Type', y='Value', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Raw Data Values Distribution')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Embedding box plot\n",
    "n_sample_emb = min(10000, adata_pred_emb.X.shape[0] * adata_pred_emb.X.shape[1])\n",
    "pred_emb_flat = adata_pred_emb.X.flatten()\n",
    "real_emb_flat = adata_real_emb.X.flatten()\n",
    "idx_sample_emb = np.random.choice(len(pred_emb_flat), n_sample_emb, replace=False)\n",
    "\n",
    "emb_data_for_box = pd.DataFrame({\n",
    "    'Value': np.concatenate([pred_emb_flat[idx_sample_emb], real_emb_flat[idx_sample_emb]]),\n",
    "    'Type': ['Predicted']*n_sample_emb + ['Real']*n_sample_emb\n",
    "})\n",
    "\n",
    "sns.boxplot(data=emb_data_for_box, x='Type', y='Value', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Embedding Values Distribution')\n",
    "\n",
    "# L2 norm box plots\n",
    "norm_data_raw = pd.DataFrame({\n",
    "    'L2_Norm': np.concatenate([pred_raw_norms, real_raw_norms]),\n",
    "    'Type': ['Predicted']*len(pred_raw_norms) + ['Real']*len(real_raw_norms)\n",
    "})\n",
    "\n",
    "sns.boxplot(data=norm_data_raw, x='Type', y='L2_Norm', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('L2 Norms - Raw Data')\n",
    "\n",
    "norm_data_emb = pd.DataFrame({\n",
    "    'L2_Norm': np.concatenate([pred_emb_norms, real_emb_norms]),\n",
    "    'Type': ['Predicted']*len(pred_emb_norms) + ['Real']*len(real_emb_norms)\n",
    "})\n",
    "\n",
    "sns.boxplot(data=norm_data_emb, x='Type', y='L2_Norm', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('L2 Norms - Embeddings')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('box_plot_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved box plot comparisons to 'box_plot_comparison.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy but with .obsm['X_vci_1.5.2_4'] in the .X\n",
    "adata_pred_emb = ad.AnnData(X=adata_pred.obsm['X_vci_1.5.2_4'], obs=adata_pred.obs)\n",
    "adata_real_emb = ad.AnnData(X=adata_real.obsm['X_vci_1.5.2_4'], obs=adata_real.obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_perturbation_ranking_score_rev' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mcompute_perturbation_ranking_score_rev\u001b[49m(adata_pred, adata_real, pert_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgene\u001b[39m\u001b[38;5;124m\"\u001b[39m, ctrl_pert\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-targeting\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'compute_perturbation_ranking_score_rev' is not defined"
     ]
    }
   ],
   "source": [
    "len(compute_perturbation_ranking_score_rev(adata_pred, adata_real, pert_col=\"gene\", ctrl_pert=\"non-targeting\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COSINE SIMILARITY ANALYSIS ===\n",
      "Real data shape: (49389, 2000)\n",
      "Real embedding shape: (49389, 2058)\n",
      "Sampling 1000 vectors from 2123429 total vectors\n",
      "Sample shape: (1000, 2000)\n",
      "Computing cosine similarity matrix...\n",
      "Similarity matrix shape: (1000, 1000)\n",
      "Sampling 1000 vectors from 2123429 total vectors\n",
      "Sample shape: (1000, 2000)\n",
      "Computing cosine similarity matrix...\n",
      "Similarity matrix shape: (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Fix the cosine similarity matrix function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def create_cosine_similarity_matrix_efficient(adata, sample_size=None):\n",
    "    \"\"\"\n",
    "    Create cosine similarity matrix for the data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        The data to compute similarity for\n",
    "    sample_size : int, optional\n",
    "        If provided, randomly sample this many vectors to make computation feasible\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cosine_sim : np.ndarray\n",
    "        Cosine similarity matrix\n",
    "    indices : np.ndarray\n",
    "        Indices of sampled vectors (if sampling was used)\n",
    "    \"\"\"\n",
    "    # Get the data matrix\n",
    "    X = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size is not None and sample_size < X.shape[0]:\n",
    "        print(f\"Sampling {sample_size} vectors from {X.shape[0]} total vectors\")\n",
    "        indices = np.random.choice(X.shape[0], sample_size, replace=False)\n",
    "        X_sample = X[indices]\n",
    "        print(f\"Sample shape: {X_sample.shape}\")\n",
    "    else:\n",
    "        X_sample = X\n",
    "        indices = np.arange(X.shape[0])\n",
    "        print(f\"Using all {X.shape[0]} vectors\")\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    print(\"Computing cosine similarity matrix...\")\n",
    "    cosine_sim = cosine_similarity(X_sample)\n",
    "    print(f\"Similarity matrix shape: {cosine_sim.shape}\")\n",
    "    \n",
    "    return cosine_sim, indices\n",
    "\n",
    "# Let's start with a smaller sample to test\n",
    "print(\"=== COSINE SIMILARITY ANALYSIS ===\")\n",
    "print(f\"Real data shape: {adata_real.X.shape}\")\n",
    "print(f\"Real embedding shape: {adata_real_emb.X.shape}\")\n",
    "\n",
    "# Start with a sample of 1000 vectors for raw data\n",
    "cosine_sim_real_sample, indices_real = create_cosine_similarity_matrix_efficient(adata_real_hop62, sample_size=1000)\n",
    "cosine_sim_pred_sample, indices_pred = create_cosine_similarity_matrix_efficient(adata_pred_hop62, sample_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COSINE SIMILARITY MATRIX STATISTICS ===\n",
      "Matrix shape: (1000, 1000)\n",
      "Mean similarity: 0.551723\n",
      "Std similarity: 0.162005\n",
      "Min similarity: 0.000000\n",
      "Max similarity: 1.000000\n",
      "Diagonal values (should be ~1.0): mean=1.000000, std=0.000000\n",
      "Off-diagonal similarities: mean=0.551274, std=0.161464\n",
      "Off-diagonal similarity percentiles:\n",
      "   1%: 0.116880\n",
      "   5%: 0.245423\n",
      "  10%: 0.337756\n",
      "  25%: 0.456606\n",
      "  50%: 0.565094\n",
      "  75%: 0.662877\n",
      "  90%: 0.748588\n",
      "  95%: 0.794463\n",
      "  99%: 0.868696\n",
      "\n",
      "Most similar pair (excluding self): 0.991899\n",
      "Least similar pair: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Analyze the cosine similarity matrix\n",
    "print(\"\\n=== COSINE SIMILARITY MATRIX STATISTICS ===\")\n",
    "print(f\"Matrix shape: {cosine_sim_pred_sample.shape}\")\n",
    "print(f\"Mean similarity: {cosine_sim_pred_sample.mean():.6f}\")\n",
    "print(f\"Std similarity: {cosine_sim_pred_sample.std():.6f}\")\n",
    "print(f\"Min similarity: {cosine_sim_pred_sample.min():.6f}\")\n",
    "print(f\"Max similarity: {cosine_sim_pred_sample.max():.6f}\")\n",
    "\n",
    "# Diagonal should be 1.0 (self-similarity)\n",
    "diagonal_values = np.diag(cosine_sim_pred_sample)\n",
    "print(f\"Diagonal values (should be ~1.0): mean={diagonal_values.mean():.6f}, std={diagonal_values.std():.6f}\")\n",
    "\n",
    "# Off-diagonal similarities (excluding self-similarity)\n",
    "mask = ~np.eye(cosine_sim_pred_sample.shape[0], dtype=bool)\n",
    "off_diagonal = cosine_sim_pred_sample[mask]\n",
    "print(f\"Off-diagonal similarities: mean={off_diagonal.mean():.6f}, std={off_diagonal.std():.6f}\")\n",
    "\n",
    "# Look at distribution of similarities\n",
    "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "print(f\"Off-diagonal similarity percentiles:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(off_diagonal, p)\n",
    "    print(f\"  {p:2d}%: {val:.6f}\")\n",
    "\n",
    "# Find most and least similar pairs\n",
    "flat_indices = np.unravel_index(np.argsort(cosine_sim_pred_sample, axis=None), cosine_sim_pred_sample.shape)\n",
    "# Exclude diagonal elements for finding extremes\n",
    "off_diag_flat = cosine_sim_pred_sample[mask]\n",
    "sorted_off_diag_idx = np.argsort(off_diag_flat)\n",
    "\n",
    "print(f\"\\nMost similar pair (excluding self): {off_diag_flat[sorted_off_diag_idx[-1]]:.6f}\")\n",
    "print(f\"Least similar pair: {off_diag_flat[sorted_off_diag_idx[0]]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COSINE SIMILARITY MATRIX STATISTICS ===\n",
      "Matrix shape: (1000, 1000)\n",
      "Mean similarity: 0.383357\n",
      "Std similarity: 0.115561\n",
      "Min similarity: 0.000000\n",
      "Max similarity: 1.000000\n",
      "Diagonal values (should be ~1.0): mean=1.000000, std=0.000000\n",
      "Off-diagonal similarities: mean=0.382739, std=0.113959\n",
      "Off-diagonal similarity percentiles:\n",
      "   1%: 0.092539\n",
      "   5%: 0.173939\n",
      "  10%: 0.229450\n",
      "  25%: 0.311508\n",
      "  50%: 0.390172\n",
      "  75%: 0.463218\n",
      "  90%: 0.523439\n",
      "  95%: 0.556829\n",
      "  99%: 0.615974\n",
      "\n",
      "Most similar pair (excluding self): 0.762782\n",
      "Least similar pair: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Analyze the cosine similarity matrix\n",
    "print(\"\\n=== COSINE SIMILARITY MATRIX STATISTICS ===\")\n",
    "print(f\"Matrix shape: {cosine_sim_real_sample.shape}\")\n",
    "print(f\"Mean similarity: {cosine_sim_real_sample.mean():.6f}\")\n",
    "print(f\"Std similarity: {cosine_sim_real_sample.std():.6f}\")\n",
    "print(f\"Min similarity: {cosine_sim_real_sample.min():.6f}\")\n",
    "print(f\"Max similarity: {cosine_sim_real_sample.max():.6f}\")\n",
    "\n",
    "# Diagonal should be 1.0 (self-similarity)\n",
    "diagonal_values = np.diag(cosine_sim_real_sample)\n",
    "print(f\"Diagonal values (should be ~1.0): mean={diagonal_values.mean():.6f}, std={diagonal_values.std():.6f}\")\n",
    "\n",
    "# Off-diagonal similarities (excluding self-similarity)\n",
    "mask = ~np.eye(cosine_sim_real_sample.shape[0], dtype=bool)\n",
    "off_diagonal = cosine_sim_real_sample[mask]\n",
    "print(f\"Off-diagonal similarities: mean={off_diagonal.mean():.6f}, std={off_diagonal.std():.6f}\")\n",
    "\n",
    "# Look at distribution of similarities\n",
    "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
    "print(f\"Off-diagonal similarity percentiles:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(off_diagonal, p)\n",
    "    print(f\"  {p:2d}%: {val:.6f}\")\n",
    "\n",
    "# Find most and least similar pairs\n",
    "flat_indices = np.unravel_index(np.argsort(cosine_sim_real_sample, axis=None), cosine_sim_real_sample.shape)\n",
    "# Exclude diagonal elements for finding extremes\n",
    "off_diag_flat = cosine_sim_real_sample[mask]\n",
    "sorted_off_diag_idx = np.argsort(off_diag_flat)\n",
    "\n",
    "print(f\"\\nMost similar pair (excluding self): {off_diag_flat[sorted_off_diag_idx[-1]]:.6f}\")\n",
    "print(f\"Least similar pair: {off_diag_flat[sorted_off_diag_idx[0]]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COSINE SIMILARITY FOR EMBEDDINGS ===\n",
      "Sampling 2000 vectors from 49389 total vectors\n",
      "Sample shape: (2000, 2058)\n",
      "Computing cosine similarity matrix...\n",
      "Similarity matrix shape: (2000, 2000)\n",
      "\n",
      "=== EMBEDDING COSINE SIMILARITY STATISTICS ===\n",
      "Matrix shape: (2000, 2000)\n",
      "Mean similarity: 0.891477\n",
      "Std similarity: 0.034753\n",
      "Min similarity: 0.630303\n",
      "Max similarity: 1.000000\n",
      "Off-diagonal similarities: mean=0.891423, std=0.034677\n",
      "Embedding off-diagonal similarity percentiles:\n",
      "   1%: 0.764490\n",
      "   5%: 0.823949\n",
      "  10%: 0.849600\n",
      "  25%: 0.877906\n",
      "  50%: 0.898541\n",
      "  75%: 0.914051\n",
      "  90%: 0.925813\n",
      "  95%: 0.931985\n",
      "  99%: 0.942137\n",
      "\n",
      "Most similar embedding pair (excluding self): 0.970238\n",
      "Least similar embedding pair: 0.630303\n"
     ]
    }
   ],
   "source": [
    "# Now let's do the same for embeddings (which should be faster due to lower dimensionality)\n",
    "print(\"\\n=== COSINE SIMILARITY FOR EMBEDDINGS ===\")\n",
    "cosine_sim_emb_sample, indices_emb = create_cosine_similarity_matrix_efficient(adata_real_emb, sample_size=2000)\n",
    "\n",
    "print(\"\\n=== EMBEDDING COSINE SIMILARITY STATISTICS ===\")\n",
    "print(f\"Matrix shape: {cosine_sim_emb_sample.shape}\")\n",
    "print(f\"Mean similarity: {cosine_sim_emb_sample.mean():.6f}\")\n",
    "print(f\"Std similarity: {cosine_sim_emb_sample.std():.6f}\")\n",
    "print(f\"Min similarity: {cosine_sim_emb_sample.min():.6f}\")\n",
    "print(f\"Max similarity: {cosine_sim_emb_sample.max():.6f}\")\n",
    "\n",
    "# Off-diagonal similarities for embeddings\n",
    "mask_emb = ~np.eye(cosine_sim_emb_sample.shape[0], dtype=bool)\n",
    "off_diagonal_emb = cosine_sim_emb_sample[mask_emb]\n",
    "print(f\"Off-diagonal similarities: mean={off_diagonal_emb.mean():.6f}, std={off_diagonal_emb.std():.6f}\")\n",
    "\n",
    "print(f\"Embedding off-diagonal similarity percentiles:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(off_diagonal_emb, p)\n",
    "    print(f\"  {p:2d}%: {val:.6f}\")\n",
    "\n",
    "off_diag_emb_flat = cosine_sim_emb_sample[mask_emb]\n",
    "sorted_off_diag_emb_idx = np.argsort(off_diag_emb_flat)\n",
    "\n",
    "print(f\"\\nMost similar embedding pair (excluding self): {off_diag_emb_flat[sorted_off_diag_emb_idx[-1]]:.6f}\")\n",
    "print(f\"Least similar embedding pair: {off_diag_emb_flat[sorted_off_diag_emb_idx[0]]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EUCLIDEAN DISTANCE ANALYSIS ===\n",
      "Sampling 1000 vectors from 49389 total vectors\n",
      "Sample shape: (1000, 2000)\n",
      "Computing Euclidean distance matrix...\n",
      "Distance matrix shape: (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Create Euclidean distance matrix function\n",
    "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances\n",
    "\n",
    "def create_euclidean_distance_matrix_efficient(adata, sample_size=None):\n",
    "    \"\"\"\n",
    "    Create Euclidean distance matrix for the data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        The data to compute distances for\n",
    "    sample_size : int, optional\n",
    "        If provided, randomly sample this many vectors to make computation feasible\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    euclidean_dist : np.ndarray\n",
    "        Euclidean distance matrix\n",
    "    indices : np.ndarray\n",
    "        Indices of sampled vectors (if sampling was used)\n",
    "    \"\"\"\n",
    "    # Get the data matrix\n",
    "    X = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size is not None and sample_size < X.shape[0]:\n",
    "        print(f\"Sampling {sample_size} vectors from {X.shape[0]} total vectors\")\n",
    "        indices = np.random.choice(X.shape[0], sample_size, replace=False)\n",
    "        X_sample = X[indices]\n",
    "        print(f\"Sample shape: {X_sample.shape}\")\n",
    "    else:\n",
    "        X_sample = X\n",
    "        indices = np.arange(X.shape[0])\n",
    "        print(f\"Using all {X.shape[0]} vectors\")\n",
    "    \n",
    "    # Compute Euclidean distance matrix\n",
    "    print(\"Computing Euclidean distance matrix...\")\n",
    "    euclidean_dist = manhattan_distances(X_sample)\n",
    "    print(f\"Distance matrix shape: {euclidean_dist.shape}\")\n",
    "    \n",
    "    return euclidean_dist, indices\n",
    "\n",
    "# Compute Euclidean distance matrices for the same samples\n",
    "print(\"=== EUCLIDEAN DISTANCE ANALYSIS ===\")\n",
    "\n",
    "# Use same random seed for fair comparison\n",
    "np.random.seed(42)\n",
    "euclidean_dist_real_sample, _ = create_euclidean_distance_matrix_efficient(adata_real, sample_size=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EUCLIDEAN DISTANCE MATRIX STATISTICS ===\n",
      "Matrix shape: (1000, 1000)\n",
      "Mean distance: 783.013035\n",
      "Std distance: 81.163833\n",
      "Min distance: 0.000000\n",
      "Max distance: 1202.497559\n",
      "Diagonal values (should be ~0.0): mean=0.000000, std=0.000000\n",
      "Off-diagonal distances: mean=783.796831, std=77.329325\n",
      "Off-diagonal distance percentiles:\n",
      "   1%: 608.730352\n",
      "   5%: 665.031250\n",
      "  10%: 692.349829\n",
      "  25%: 734.325439\n",
      "  50%: 779.271118\n",
      "  75%: 828.544922\n",
      "  90%: 880.565527\n",
      "  95%: 917.891187\n",
      "  99%: 1000.234805\n",
      "\n",
      "Most distant pair: 1202.497559\n",
      "Least distant pair (excluding self): 418.563965\n",
      "Sampling 2000 vectors from 49389 total vectors\n",
      "Sample shape: (2000, 2058)\n",
      "Computing Euclidean distance matrix...\n",
      "Distance matrix shape: (2000, 2000)\n",
      "\n",
      "=== EMBEDDING EUCLIDEAN DISTANCE STATISTICS ===\n",
      "Off-diagonal distances: mean=27.331081, std=3.417234\n",
      "Embedding off-diagonal distance percentiles:\n",
      "   1%: 20.819709\n",
      "   5%: 22.538638\n",
      "  10%: 23.455931\n",
      "  25%: 25.007704\n",
      "  50%: 26.882766\n",
      "  75%: 29.149899\n",
      "  90%: 31.904585\n",
      "  95%: 33.818535\n",
      "  99%: 37.639088\n",
      "\n",
      "Most distant embedding pair: 44.449306\n",
      "Least distant embedding pair (excluding self): 13.857272\n"
     ]
    }
   ],
   "source": [
    "# Analyze the Euclidean distance matrix\n",
    "print(\"\\n=== EUCLIDEAN DISTANCE MATRIX STATISTICS ===\")\n",
    "print(f\"Matrix shape: {euclidean_dist_real_sample.shape}\")\n",
    "print(f\"Mean distance: {euclidean_dist_real_sample.mean():.6f}\")\n",
    "print(f\"Std distance: {euclidean_dist_real_sample.std():.6f}\")\n",
    "print(f\"Min distance: {euclidean_dist_real_sample.min():.6f}\")\n",
    "print(f\"Max distance: {euclidean_dist_real_sample.max():.6f}\")\n",
    "\n",
    "# Diagonal should be 0.0 (self-distance)\n",
    "diagonal_values_dist = np.diag(euclidean_dist_real_sample)\n",
    "print(f\"Diagonal values (should be ~0.0): mean={diagonal_values_dist.mean():.6f}, std={diagonal_values_dist.std():.6f}\")\n",
    "\n",
    "# Off-diagonal distances (excluding self-distance)\n",
    "mask_dist = ~np.eye(euclidean_dist_real_sample.shape[0], dtype=bool)\n",
    "off_diagonal_dist = euclidean_dist_real_sample[mask_dist]\n",
    "print(f\"Off-diagonal distances: mean={off_diagonal_dist.mean():.6f}, std={off_diagonal_dist.std():.6f}\")\n",
    "\n",
    "# Look at distribution of distances\n",
    "print(f\"Off-diagonal distance percentiles:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(off_diagonal_dist, p)\n",
    "    print(f\"  {p:2d}%: {val:.6f}\")\n",
    "\n",
    "# Find most and least distant pairs\n",
    "sorted_off_diag_dist_idx = np.argsort(off_diagonal_dist)\n",
    "\n",
    "print(f\"\\nMost distant pair: {off_diagonal_dist[sorted_off_diag_dist_idx[-1]]:.6f}\")\n",
    "print(f\"Least distant pair (excluding self): {off_diagonal_dist[sorted_off_diag_dist_idx[0]]:.6f}\")\n",
    "\n",
    "# Also compute for embeddings\n",
    "np.random.seed(42)\n",
    "euclidean_dist_emb_sample, _ = create_euclidean_distance_matrix_efficient(adata_real_emb, sample_size=2000)\n",
    "\n",
    "print(\"\\n=== EMBEDDING EUCLIDEAN DISTANCE STATISTICS ===\")\n",
    "mask_emb_dist = ~np.eye(euclidean_dist_emb_sample.shape[0], dtype=bool)\n",
    "off_diagonal_emb_dist = euclidean_dist_emb_sample[mask_emb_dist]\n",
    "print(f\"Off-diagonal distances: mean={off_diagonal_emb_dist.mean():.6f}, std={off_diagonal_emb_dist.std():.6f}\")\n",
    "\n",
    "print(f\"Embedding off-diagonal distance percentiles:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(off_diagonal_emb_dist, p)\n",
    "    print(f\"  {p:2d}%: {val:.6f}\")\n",
    "\n",
    "sorted_off_diag_emb_dist_idx = np.argsort(off_diagonal_emb_dist)\n",
    "print(f\"\\nMost distant embedding pair: {off_diagonal_emb_dist[sorted_off_diag_emb_dist_idx[-1]]:.6f}\")\n",
    "print(f\"Least distant embedding pair (excluding self): {off_diagonal_emb_dist[sorted_off_diag_emb_dist_idx[0]]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3168865/238431976.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3168865/238431976.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29083150816367015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perturbation_ranking_score(adata_pred_emb, adata_real_emb, pert_col=\"gene\", ctrl_pert=\"non-targeting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the Euclidean distance function and compute for embeddings\n",
    "def create_euclidean_distance_matrix_efficient(adata, sample_size=None):\n",
    "    \"\"\"\n",
    "    Create Euclidean distance matrix for the data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        The data to compute distances for\n",
    "    sample_size : int, optional\n",
    "        If provided, randomly sample this many vectors to make computation feasible\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    euclidean_dist : np.ndarray\n",
    "        Euclidean distance matrix\n",
    "    indices : np.ndarray\n",
    "        Indices of sampled vectors (if sampling was used)\n",
    "    \"\"\"\n",
    "    # Get the data matrix\n",
    "    X = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size is not None and sample_size < X.shape[0]:\n",
    "        print(f\"Sampling {sample_size} vectors from {X.shape[0]} total vectors\")\n",
    "        indices = np.random.choice(X.shape[0], sample_size, replace=False)\n",
    "        X_sample = X[indices]\n",
    "        print(f\"Sample shape: {X_sample.shape}\")\n",
    "    else:\n",
    "        X_sample = X\n",
    "        indices = np.arange(X.shape[0])\n",
    "        print(f\"Using all {X.shape[0]} vectors\")\n",
    "    \n",
    "    # Compute Euclidean distance matrix\n",
    "    print(\"Computing Euclidean distance matrix...\")\n",
    "    euclidean_dist = euclidean_distances(X_sample)\n",
    "    print(f\"Distance matrix shape: {euclidean_dist.shape}\")\n",
    "    \n",
    "    return euclidean_dist, indices\n",
    "\n",
    "# Compute Euclidean distances for embeddings\n",
    "print(\"=== EUCLIDEAN DISTANCE FOR EMBEDDINGS ===\")\n",
    "euclidean_dist_emb_sample, indices_emb_dist = create_euclidean_distance_matrix_efficient(adata_real_emb, sample_size=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the Euclidean distance matrix for embeddings\n",
    "print(\"\\n=== EUCLIDEAN DISTANCE MATRIX STATISTICS (EMBEDDINGS) ===\")\n",
    "print(f\"Matrix shape: {euclidean_dist_emb_sample.shape}\")\n",
    "print(f\"Mean distance: {euclidean_dist_emb_sample.mean():.6f}\")\n",
    "print(f\"Std distance: {euclidean_dist_emb_sample.std():.6f}\")\n",
    "print(f\"Min distance: {euclidean_dist_emb_sample.min():.6f}\")\n",
    "print(f\"Max distance: {euclidean_dist_emb_sample.max():.6f}\")\n",
    "\n",
    "# Diagonal should be 0.0 (self-distance)\n",
    "diagonal_values_dist_emb = np.diag(euclidean_dist_emb_sample)\n",
    "print(f\"Diagonal values (should be ~0.0): mean={diagonal_values_dist_emb.mean():.6f}, std={diagonal_values_dist_emb.std():.6f}\")\n",
    "\n",
    "# Off-diagonal distances (excluding self-distance)\n",
    "mask_dist_emb = ~np.eye(euclidean_dist_emb_sample.shape[0], dtype=bool)\n",
    "off_diagonal_dist_emb = euclidean_dist_emb_sample[mask_dist_emb]\n",
    "print(f\"Off-diagonal distances: mean={off_diagonal_dist_emb.mean():.6f}, std={off_diagonal_dist_emb.std():.6f}\")\n",
    "\n",
    "# Look at distribution of distances\n",
    "print(f\"Embedding off-diagonal distance percentiles:\")\n",
    "for p in percentiles:\n",
    "    val = np.percentile(off_diagonal_dist_emb, p)\n",
    "    print(f\"  {p:2d}%: {val:.6f}\")\n",
    "\n",
    "# Find closest and farthest pairs\n",
    "sorted_off_diag_dist_emb_idx = np.argsort(off_diagonal_dist_emb)\n",
    "\n",
    "print(f\"\\nClosest embedding pair (excluding self): {off_diagonal_dist_emb[sorted_off_diag_dist_emb_idx[0]]:.6f}\")\n",
    "print(f\"Farthest embedding pair: {off_diagonal_dist_emb[sorted_off_diag_dist_emb_idx[-1]]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Cosine Similarity vs Euclidean Distance for embeddings\n",
    "print(\"\\n=== COSINE SIMILARITY vs EUCLIDEAN DISTANCE COMPARISON (EMBEDDINGS) ===\")\n",
    "\n",
    "# Calculate correlation between cosine similarity and Euclidean distance\n",
    "# Note: We need to use the same sample indices for fair comparison\n",
    "cosine_sim_values = cosine_sim_emb_sample[mask_emb]\n",
    "euclidean_dist_values = euclidean_dist_emb_sample[mask_dist_emb]\n",
    "\n",
    "correlation = np.corrcoef(cosine_sim_values, euclidean_dist_values)[0, 1]\n",
    "print(f\"Correlation between cosine similarity and Euclidean distance: {correlation:.6f}\")\n",
    "\n",
    "# Convert cosine similarity to cosine distance for better comparison\n",
    "cosine_distance_values = 1 - cosine_sim_values\n",
    "correlation_cosine_dist = np.corrcoef(cosine_distance_values, euclidean_dist_values)[0, 1]\n",
    "print(f\"Correlation between cosine distance and Euclidean distance: {correlation_cosine_dist:.6f}\")\n",
    "\n",
    "print(f\"\\nSummary comparison:\")\n",
    "print(f\"Cosine similarity: mean={cosine_sim_values.mean():.6f}, std={cosine_sim_values.std():.6f}\")\n",
    "print(f\"Cosine distance:   mean={cosine_distance_values.mean():.6f}, std={cosine_distance_values.std():.6f}\")\n",
    "print(f\"Euclidean distance: mean={euclidean_dist_values.mean():.6f}, std={euclidean_dist_values.std():.6f}\")\n",
    "\n",
    "# Normalize Euclidean distances for comparison\n",
    "euclidean_normalized = (euclidean_dist_values - euclidean_dist_values.min()) / (euclidean_dist_values.max() - euclidean_dist_values.min())\n",
    "print(f\"Euclidean (normalized): mean={euclidean_normalized.mean():.6f}, std={euclidean_normalized.std():.6f}\")\n",
    "\n",
    "correlation_normalized = np.corrcoef(cosine_distance_values, euclidean_normalized)[0, 1]\n",
    "print(f\"Correlation between cosine distance and normalized Euclidean distance: {correlation_normalized:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization comparing cosine similarity and Euclidean distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Heatmap of Euclidean distance matrix (embeddings)\n",
    "im1 = axes[0, 0].imshow(euclidean_dist_emb_sample, cmap='viridis', aspect='auto')\n",
    "axes[0, 0].set_title('Euclidean Distance Matrix - Embeddings')\n",
    "axes[0, 0].set_xlabel('Sample Index')\n",
    "axes[0, 0].set_ylabel('Sample Index')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# Heatmap of cosine similarity matrix (embeddings) for comparison\n",
    "im2 = axes[0, 1].imshow(cosine_sim_emb_sample, cmap='viridis', aspect='auto')\n",
    "axes[0, 1].set_title('Cosine Similarity Matrix - Embeddings')\n",
    "axes[0, 1].set_xlabel('Sample Index')\n",
    "axes[0, 1].set_ylabel('Sample Index')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# Scatter plot: Cosine similarity vs Euclidean distance\n",
    "# Sample a subset for visualization\n",
    "n_sample_vis = min(10000, len(cosine_sim_values))\n",
    "sample_idx_vis = np.random.choice(len(cosine_sim_values), n_sample_vis, replace=False)\n",
    "\n",
    "axes[0, 2].scatter(cosine_sim_values[sample_idx_vis], euclidean_dist_values[sample_idx_vis], \n",
    "                   alpha=0.5, s=1)\n",
    "axes[0, 2].set_xlabel('Cosine Similarity')\n",
    "axes[0, 2].set_ylabel('Euclidean Distance')\n",
    "axes[0, 2].set_title(f'Cosine Sim vs Euclidean Dist\\n(corr={correlation:.3f})')\n",
    "\n",
    "# Distribution of Euclidean distances\n",
    "axes[1, 0].hist(euclidean_dist_values, bins=100, alpha=0.7, density=True, color='green')\n",
    "axes[1, 0].set_title('Euclidean Distance Distribution - Embeddings')\n",
    "axes[1, 0].set_xlabel('Distance')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "\n",
    "# Distribution of cosine similarities (for comparison)\n",
    "axes[1, 1].hist(cosine_sim_values, bins=100, alpha=0.7, density=True, color='blue')\n",
    "axes[1, 1].set_title('Cosine Similarity Distribution - Embeddings')\n",
    "axes[1, 1].set_xlabel('Similarity')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "\n",
    "# Combined distributions (normalized)\n",
    "axes[1, 2].hist(cosine_distance_values, bins=100, alpha=0.6, density=True, \n",
    "                color='blue', label='Cosine Distance')\n",
    "axes[1, 2].hist(euclidean_normalized, bins=100, alpha=0.6, density=True, \n",
    "                color='green', label='Euclidean (normalized)')\n",
    "axes[1, 2].set_title('Distance Distributions Comparison')\n",
    "axes[1, 2].set_xlabel('Distance')\n",
    "axes[1, 2].set_ylabel('Density')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('embedding_similarity_distance_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved as 'embedding_similarity_distance_comparison.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== KL DIVERGENCE FOR EMBEDDINGS ===\n",
      "Sampling 500 vectors from 49389 total vectors\n",
      "Sample shape: (500, 2058)\n",
      "Computing KL divergence matrix...\n",
      "Processing sample 0/500\n",
      "Processing sample 100/500\n",
      "Processing sample 200/500\n",
      "Processing sample 300/500\n",
      "Processing sample 400/500\n",
      "KL divergence matrix shape: (500, 500)\n"
     ]
    }
   ],
   "source": [
    "# Create KL divergence matrix function\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def create_kl_divergence_matrix_efficient(adata, sample_size=None, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Create KL divergence matrix for the data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata : AnnData\n",
    "        The data to compute KL divergences for\n",
    "    sample_size : int, optional\n",
    "        If provided, randomly sample this many vectors to make computation feasible\n",
    "    epsilon : float\n",
    "        Small value to add for numerical stability\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    kl_div_matrix : np.ndarray\n",
    "        KL divergence matrix\n",
    "    indices : np.ndarray\n",
    "        Indices of sampled vectors (if sampling was used)\n",
    "    \"\"\"\n",
    "    # Get the data matrix\n",
    "    X = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size is not None and sample_size < X.shape[0]:\n",
    "        print(f\"Sampling {sample_size} vectors from {X.shape[0]} total vectors\")\n",
    "        indices = np.random.choice(X.shape[0], sample_size, replace=False)\n",
    "        X_sample = X[indices]\n",
    "        print(f\"Sample shape: {X_sample.shape}\")\n",
    "    else:\n",
    "        X_sample = X\n",
    "        indices = np.arange(X.shape[0])\n",
    "        print(f\"Using all {X.shape[0]} vectors\")\n",
    "    \n",
    "    # Convert to probability distributions\n",
    "    # First, handle negative values by shifting to positive\n",
    "    X_shifted = X_sample - X_sample.min(axis=1, keepdims=True) + epsilon\n",
    "    \n",
    "    # Normalize to probability distributions (sum to 1)\n",
    "    X_prob = X_shifted / X_shifted.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    print(\"Computing KL divergence matrix...\")\n",
    "    n_samples = X_prob.shape[0]\n",
    "    kl_div_matrix = np.zeros((n_samples, n_samples))\n",
    "    \n",
    "    # Compute KL divergence for each pair\n",
    "    for i in range(n_samples):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing sample {i}/{n_samples}\")\n",
    "        for j in range(n_samples):\n",
    "            if i == j:\n",
    "                kl_div_matrix[i, j] = 0.0  # KL(P||P) = 0\n",
    "            else:\n",
    "                # KL(P||Q) = sum(P * log(P/Q))\n",
    "                p = X_prob[i]\n",
    "                q = X_prob[j]\n",
    "                # Add small epsilon to q to avoid division by zero\n",
    "                q_safe = q + epsilon\n",
    "                kl_div = entropy(p, q_safe)\n",
    "                kl_div_matrix[i, j] = kl_div\n",
    "    \n",
    "    print(f\"KL divergence matrix shape: {kl_div_matrix.shape}\")\n",
    "    return kl_div_matrix, indices\n",
    "\n",
    "# Compute KL divergence for embeddings\n",
    "print(\"=== KL DIVERGENCE FOR EMBEDDINGS ===\")\n",
    "kl_div_emb_sample, indices_kl_emb = create_kl_divergence_matrix_efficient(adata_real_emb, sample_size=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KL DIVERGENCE MATRIX STATISTICS (EMBEDDINGS) ===\n",
      "Matrix shape: (500, 500)\n",
      "Mean KL divergence: inf\n",
      "Std KL divergence: nan\n",
      "Min KL divergence: -0.000567\n",
      "Max KL divergence: inf\n",
      "Number of infinite values: 105592\n",
      "Number of NaN values: 0\n",
      "Diagonal values (should be ~0.0): mean=0.000000, std=0.000000\n",
      "Off-diagonal KL divergences (finite only): mean=0.000364, std=0.000339\n",
      "Number of finite off-diagonal values: 143908 out of 249500\n",
      "Embedding off-diagonal KL divergence percentiles:\n",
      "   1%: -0.000329\n",
      "   5%: -0.000179\n",
      "  10%: -0.000076\n",
      "  25%: 0.000125\n",
      "  50%: 0.000353\n",
      "  75%: 0.000591\n",
      "  90%: 0.000808\n",
      "  95%: 0.000927\n",
      "  99%: 0.001223\n",
      "\n",
      "Most similar embedding pair (lowest KL divergence): -0.000567\n",
      "Most divergent embedding pair (highest finite KL divergence): 0.002168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruvgautam/miniconda/envs/pertsets/lib/python3.12/site-packages/numpy/core/_methods.py:173: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n"
     ]
    }
   ],
   "source": [
    "# Analyze the KL divergence matrix for embeddings\n",
    "print(\"\\n=== KL DIVERGENCE MATRIX STATISTICS (EMBEDDINGS) ===\")\n",
    "print(f\"Matrix shape: {kl_div_emb_sample.shape}\")\n",
    "print(f\"Mean KL divergence: {kl_div_emb_sample.mean():.6f}\")\n",
    "print(f\"Std KL divergence: {kl_div_emb_sample.std():.6f}\")\n",
    "print(f\"Min KL divergence: {kl_div_emb_sample.min():.6f}\")\n",
    "print(f\"Max KL divergence: {kl_div_emb_sample.max():.6f}\")\n",
    "\n",
    "# Check for infinite values\n",
    "n_inf = np.isinf(kl_div_emb_sample).sum()\n",
    "n_nan = np.isnan(kl_div_emb_sample).sum()\n",
    "print(f\"Number of infinite values: {n_inf}\")\n",
    "print(f\"Number of NaN values: {n_nan}\")\n",
    "\n",
    "# Diagonal should be 0.0 (KL(P||P) = 0)\n",
    "diagonal_values_kl = np.diag(kl_div_emb_sample)\n",
    "print(f\"Diagonal values (should be ~0.0): mean={diagonal_values_kl.mean():.6f}, std={diagonal_values_kl.std():.6f}\")\n",
    "\n",
    "# Off-diagonal KL divergences (excluding self-divergence)\n",
    "mask_kl = ~np.eye(kl_div_emb_sample.shape[0], dtype=bool)\n",
    "off_diagonal_kl = kl_div_emb_sample[mask_kl]\n",
    "\n",
    "# Remove infinite and NaN values for statistics\n",
    "off_diagonal_kl_finite = off_diagonal_kl[np.isfinite(off_diagonal_kl)]\n",
    "print(f\"Off-diagonal KL divergences (finite only): mean={off_diagonal_kl_finite.mean():.6f}, std={off_diagonal_kl_finite.std():.6f}\")\n",
    "print(f\"Number of finite off-diagonal values: {len(off_diagonal_kl_finite)} out of {len(off_diagonal_kl)}\")\n",
    "\n",
    "# Look at distribution of KL divergences\n",
    "if len(off_diagonal_kl_finite) > 0:\n",
    "    print(f\"Embedding off-diagonal KL divergence percentiles:\")\n",
    "    for p in percentiles:\n",
    "        val = np.percentile(off_diagonal_kl_finite, p)\n",
    "        print(f\"  {p:2d}%: {val:.6f}\")\n",
    "\n",
    "    # Find most and least divergent pairs\n",
    "    sorted_off_diag_kl_idx = np.argsort(off_diagonal_kl_finite)\n",
    "    print(f\"\\nMost similar embedding pair (lowest KL divergence): {off_diagonal_kl_finite[sorted_off_diag_kl_idx[0]]:.6f}\")\n",
    "    print(f\"Most divergent embedding pair (highest finite KL divergence): {off_diagonal_kl_finite[sorted_off_diag_kl_idx[-1]]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare KL divergence with cosine similarity and Euclidean distance\n",
    "print(\"\\n=== COMPARISON: KL DIVERGENCE vs COSINE SIMILARITY vs EUCLIDEAN DISTANCE ===\")\n",
    "\n",
    "# We need to get the same sample indices for fair comparison\n",
    "# Let's use the KL divergence sample size (500) and match the indices\n",
    "cosine_sim_kl_sample, _ = create_cosine_similarity_matrix_efficient(adata_real_emb, sample_size=500)\n",
    "euclidean_dist_kl_sample, _ = create_euclidean_distance_matrix_efficient(adata_real_emb, sample_size=500)\n",
    "\n",
    "# Get off-diagonal values for all three metrics\n",
    "mask_kl_comp = ~np.eye(kl_div_emb_sample.shape[0], dtype=bool)\n",
    "\n",
    "cosine_sim_kl_values = cosine_sim_kl_sample[mask_kl_comp]\n",
    "euclidean_dist_kl_values = euclidean_dist_kl_sample[mask_kl_comp]\n",
    "kl_div_kl_values = kl_div_emb_sample[mask_kl_comp]\n",
    "\n",
    "# Remove infinite/NaN values for correlation analysis\n",
    "finite_mask = np.isfinite(kl_div_kl_values)\n",
    "cosine_finite = cosine_sim_kl_values[finite_mask]\n",
    "euclidean_finite = euclidean_dist_kl_values[finite_mask]\n",
    "kl_finite = kl_div_kl_values[finite_mask]\n",
    "\n",
    "print(f\"Number of finite comparisons: {len(kl_finite)} out of {len(kl_div_kl_values)}\")\n",
    "\n",
    "if len(kl_finite) > 1000:  # Need sufficient data for correlation\n",
    "    # Calculate correlations\n",
    "    corr_kl_cosine = np.corrcoef(kl_finite, cosine_finite)[0, 1]\n",
    "    corr_kl_euclidean = np.corrcoef(kl_finite, euclidean_finite)[0, 1]\n",
    "    \n",
    "    # Convert cosine similarity to distance for better comparison\n",
    "    cosine_dist_finite = 1 - cosine_finite\n",
    "    corr_kl_cosine_dist = np.corrcoef(kl_finite, cosine_dist_finite)[0, 1]\n",
    "    \n",
    "    print(f\"Correlation between KL divergence and cosine similarity: {corr_kl_cosine:.6f}\")\n",
    "    print(f\"Correlation between KL divergence and cosine distance: {corr_kl_cosine_dist:.6f}\")\n",
    "    print(f\"Correlation between KL divergence and Euclidean distance: {corr_kl_euclidean:.6f}\")\n",
    "    \n",
    "    print(f\"\\nSummary statistics for comparison:\")\n",
    "    print(f\"KL divergence:     mean={kl_finite.mean():.6f}, std={kl_finite.std():.6f}\")\n",
    "    print(f\"Cosine similarity: mean={cosine_finite.mean():.6f}, std={cosine_finite.std():.6f}\")\n",
    "    print(f\"Cosine distance:   mean={cosine_dist_finite.mean():.6f}, std={cosine_dist_finite.std():.6f}\")\n",
    "    print(f\"Euclidean distance: mean={euclidean_finite.mean():.6f}, std={euclidean_finite.std():.6f}\")\n",
    "else:\n",
    "    print(\"Insufficient finite values for correlation analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization comparing all three distance/similarity metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Heatmap of KL divergence matrix (embeddings)\n",
    "# Clip extreme values for better visualization\n",
    "kl_clipped = np.clip(kl_div_emb_sample, 0, np.percentile(kl_div_emb_sample[np.isfinite(kl_div_emb_sample)], 95))\n",
    "im1 = axes[0, 0].imshow(kl_clipped, cmap='viridis', aspect='auto')\n",
    "axes[0, 0].set_title('KL Divergence Matrix - Embeddings\\n(clipped at 95th percentile)')\n",
    "axes[0, 0].set_xlabel('Sample Index')\n",
    "axes[0, 0].set_ylabel('Sample Index')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# Heatmap of cosine similarity matrix for comparison\n",
    "im2 = axes[0, 1].imshow(cosine_sim_kl_sample, cmap='viridis', aspect='auto')\n",
    "axes[0, 1].set_title('Cosine Similarity Matrix - Embeddings')\n",
    "axes[0, 1].set_xlabel('Sample Index')\n",
    "axes[0, 1].set_ylabel('Sample Index')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# Heatmap of Euclidean distance matrix for comparison\n",
    "im3 = axes[0, 2].imshow(euclidean_dist_kl_sample, cmap='viridis', aspect='auto')\n",
    "axes[0, 2].set_title('Euclidean Distance Matrix - Embeddings')\n",
    "axes[0, 2].set_xlabel('Sample Index')\n",
    "axes[0, 2].set_ylabel('Sample Index')\n",
    "plt.colorbar(im3, ax=axes[0, 2])\n",
    "\n",
    "# Scatter plot: KL divergence vs Cosine similarity (if we have enough finite values)\n",
    "if len(kl_finite) > 1000:\n",
    "    n_sample_vis = min(5000, len(kl_finite))\n",
    "    sample_idx_vis = np.random.choice(len(kl_finite), n_sample_vis, replace=False)\n",
    "    \n",
    "    axes[0, 3].scatter(cosine_finite[sample_idx_vis], kl_finite[sample_idx_vis], \n",
    "                       alpha=0.5, s=1)\n",
    "    axes[0, 3].set_xlabel('Cosine Similarity')\n",
    "    axes[0, 3].set_ylabel('KL Divergence')\n",
    "    axes[0, 3].set_title(f'Cosine Sim vs KL Divergence\\n(corr={corr_kl_cosine:.3f})')\n",
    "else:\n",
    "    axes[0, 3].text(0.5, 0.5, 'Insufficient\\nfinite values', ha='center', va='center', transform=axes[0, 3].transAxes)\n",
    "    axes[0, 3].set_title('KL vs Cosine (insufficient data)')\n",
    "\n",
    "# Distribution plots\n",
    "axes[1, 0].hist(kl_finite, bins=50, alpha=0.7, density=True, color='red')\n",
    "axes[1, 0].set_title('KL Divergence Distribution')\n",
    "axes[1, 0].set_xlabel('KL Divergence')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "axes[1, 1].hist(cosine_finite, bins=50, alpha=0.7, density=True, color='blue')\n",
    "axes[1, 1].set_title('Cosine Similarity Distribution')\n",
    "axes[1, 1].set_xlabel('Cosine Similarity')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "\n",
    "axes[1, 2].hist(euclidean_finite, bins=50, alpha=0.7, density=True, color='green')\n",
    "axes[1, 2].set_title('Euclidean Distance Distribution')\n",
    "axes[1, 2].set_xlabel('Euclidean Distance')\n",
    "axes[1, 2].set_ylabel('Density')\n",
    "\n",
    "# Combined comparison plot\n",
    "if len(kl_finite) > 1000:\n",
    "    # Normalize all metrics to [0,1] for comparison\n",
    "    kl_norm = (kl_finite - kl_finite.min()) / (kl_finite.max() - kl_finite.min())\n",
    "    cosine_dist_norm = (cosine_dist_finite - cosine_dist_finite.min()) / (cosine_dist_finite.max() - cosine_dist_finite.min())\n",
    "    euclidean_norm = (euclidean_finite - euclidean_finite.min()) / (euclidean_finite.max() - euclidean_finite.min())\n",
    "    \n",
    "    axes[1, 3].hist(kl_norm, bins=50, alpha=0.5, density=True, color='red', label='KL Divergence')\n",
    "    axes[1, 3].hist(cosine_dist_norm, bins=50, alpha=0.5, density=True, color='blue', label='Cosine Distance')\n",
    "    axes[1, 3].hist(euclidean_norm, bins=50, alpha=0.5, density=True, color='green', label='Euclidean Distance')\n",
    "    axes[1, 3].set_title('Normalized Distance Distributions')\n",
    "    axes[1, 3].set_xlabel('Normalized Distance')\n",
    "    axes[1, 3].set_ylabel('Density')\n",
    "    axes[1, 3].legend()\n",
    "else:\n",
    "    axes[1, 3].text(0.5, 0.5, 'Insufficient\\nfinite values', ha='center', va='center', transform=axes[1, 3].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('kl_divergence_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved as 'kl_divergence_comparison.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_pca_anndatas(adata_reference, adata_query, n_components=50, scale=True):\n",
    "    \"\"\"\n",
    "    Fit PCA on reference dataset and apply to both reference and query datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    adata_reference : AnnData\n",
    "        Reference dataset to fit PCA on (adata_real)\n",
    "    adata_query : AnnData  \n",
    "        Query dataset to transform using reference PCA (adata_pred)\n",
    "    n_components : int\n",
    "        Number of PCA components\n",
    "    scale : bool\n",
    "        Whether to standardize the data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (adata_reference_pca, adata_query_pca)\n",
    "    \"\"\"\n",
    "    # Get data matrices\n",
    "    X_ref = adata_reference.X.copy()\n",
    "    X_query = adata_query.X.copy()\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(X_ref, 'toarray'):\n",
    "        X_ref = X_ref.toarray()\n",
    "    if hasattr(X_query, 'toarray'):\n",
    "        X_query = X_query.toarray()\n",
    "    \n",
    "    # Fit scaler on reference data only\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_ref_scaled = scaler.fit_transform(X_ref)\n",
    "        X_query_scaled = scaler.transform(X_query)  # Use reference scaler\n",
    "    else:\n",
    "        X_ref_scaled = X_ref\n",
    "        X_query_scaled = X_query\n",
    "    \n",
    "    # Fit PCA on reference data only\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_ref_pca = pca.fit_transform(X_ref_scaled)\n",
    "    \n",
    "    # Transform query data using reference PCA\n",
    "    X_query_pca = pca.transform(X_query_scaled)\n",
    "    \n",
    "    # Create var dataframe\n",
    "    var_names = [f'PC{i+1}' for i in range(n_components)]\n",
    "    new_var = pd.DataFrame(index=var_names)\n",
    "    new_var['explained_variance_ratio'] = pca.explained_variance_ratio_\n",
    "    new_var['explained_variance'] = pca.explained_variance_\n",
    "    \n",
    "    # Create AnnData objects\n",
    "    adata_ref_pca = ad.AnnData(X=X_ref_pca, obs=adata_reference.obs.copy(), var=new_var.copy())\n",
    "    adata_query_pca = ad.AnnData(X=X_query_pca, obs=adata_query.obs.copy(), var=new_var.copy())\n",
    "    \n",
    "    # Store PCA info\n",
    "    pca_info = {\n",
    "        'components': pca.components_,\n",
    "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "        'explained_variance': pca.explained_variance_,\n",
    "        'n_components': n_components,\n",
    "        'scaled': scale,\n",
    "        'fitted_on': 'reference'\n",
    "    }\n",
    "    \n",
    "    adata_ref_pca.uns['pca'] = pca_info\n",
    "    adata_query_pca.uns['pca'] = pca_info\n",
    "    \n",
    "    return adata_ref_pca, adata_query_pca\n",
    "\n",
    "# If you want both datasets in the same PC space:\n",
    "# adata_pred_pca, adata_real_pca = create_matched_pca_anndatas(adata_pred, adata_real, n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3168865/238431976.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3168865/238431976.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26922095956017794"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_real_pca, adata_pred_pca = create_reference_pca_anndatas(adata_real, adata_pred, n_components=128)\n",
    "compute_perturbation_ranking_score(adata_pred_pca, adata_real_pca, pert_col=\"gene\", ctrl_pert=\"non-targeting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3168865/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3168865/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27592112028306665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3168865/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3168865/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33161949564590765\n"
     ]
    }
   ],
   "source": [
    "adata_real_emb_pca, adata_pred_emb_pca = create_reference_pca_anndatas(adata_real_emb, adata_pred_emb, n_components=128)\n",
    "print(compute_perturbation_ranking_score(adata_pred_emb_pca, adata_real_emb_pca, pert_col=\"gene\", ctrl_pert=\"non-targeting\"))\n",
    "print(compute_perturbation_ranking_score_rev(adata_pred_emb_pca, adata_real_emb_pca, pert_col=\"gene\", ctrl_pert=\"non-targeting\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3168865/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3168865/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.338534223449348"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perturbation_ranking_score(adata_pred_emb_min, adata_real_emb_min, pert_col=\"gene\", ctrl_pert=\"non-targeting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3168865/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3168865/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2652777834006048"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perturbation_ranking_score(adata_pred, adata_real, pert_col=\"gene\", ctrl_pert=\"non-targeting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_pred_mean = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/replogle_filtered_globalsimplesum/rpe1/eval_final.ckpt/adata_pred.h5ad')\n",
    "adata_real_mean = ad.read_h5ad('/large_storage/ctc/userspace/aadduri/preprint/replogle_filtered_globalsimplesum/rpe1/eval_final.ckpt/adata_real.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3173493/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3173493/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24906379929476255"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perturbation_ranking_score(adata_pred_mean, adata_real_mean), compute_perturbation_ranking_score_rev(adata_pred_mean, adata_real_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3173493/2320155802.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3173493/2320155802.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`x` and `y` must have the same length along `axis`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3173493/1806884317.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_perturbation_ranking_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata_pred_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madata_real_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_perturbation_ranking_score_rev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata_pred_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madata_real_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3173493/2320155802.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(adata_pred, adata_real, pert_col, ctrl_pert)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mreal_effect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_real_effect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpert\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpred_effects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_pred_effect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Compute cosine similarities between the real effect and all predicted effects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpearsonr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_effect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_effects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Get the rank of the true perturbation based on similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtrue_pert_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_perts\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/pertsets/lib/python3.12/site-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, alternative, method, axis)\u001b[0m\n\u001b[1;32m   4545\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis_int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4547\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4549\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`x` and `y` must have the same length along `axis`.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4552\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`x` and `y` must have length at least 2.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `x` and `y` must have the same length along `axis`."
     ]
    }
   ],
   "source": [
    "compute_perturbation_ranking_score(adata_pred_mean, adata_real_mean), compute_perturbation_ranking_score_rev(adata_pred_mean, adata_real_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3173493/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3173493/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3173493/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n",
      "/tmp/ipykernel_3173493/856578241.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  mean_df = adata_df.groupby(\"pert\").mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.29083150816367015, 0.3407811051419179)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_perturbation_ranking_score(adata_pred_emb, adata_real_emb), compute_perturbation_ranking_score_rev(adata_pred_emb, adata_real_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pertsets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
